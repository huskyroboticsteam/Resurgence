#include "ObjectDetector.h"

#include <algorithm>
#include <iostream>
#include <map>
#include <stdexcept>

#include <opencv2/calib3d.hpp>
#include <opencv2/imgproc.hpp>

namespace ObjDet {

ObjectDetector::ObjectDetector() 
    : confidence_threshold_(0.5f), 
      enabled_(true),
      device_(torch::kCPU) {
}

ObjectDetector::ObjectDetector(const std::vector<std::string>& class_names,
                               const std::string& model_path,
                               float confidence_threshold,
                               const cam::CameraParams& camera_params)
    : class_names_(class_names),
      camera_params_(camera_params),
      confidence_threshold_(confidence_threshold),
      enabled_(true),
      device_(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU) {
    
    // Load the model
    try {
        model_ = torch::jit::load(model_path);
        model_.to(device_);
        model_.eval();
        std::cout << "ObjectDetector: Model loaded successfully from " << model_path << std::endl;
        std::cout << "ObjectDetector: Using device: " << (device_.is_cuda() ? "CUDA" : "CPU") << std::endl;
    } catch (const c10::Error& e) {
        std::cerr << "ObjectDetector: Error loading model from " << model_path << std::endl;
        std::cerr << e.what() << std::endl;
        throw std::runtime_error("Failed to load OWL-ViT model");
    }
    
    // Initialize hard-coded tokens for class names
    // These tokens correspond to: ["no object", "orange mallet or hammer", "water bottle"]
    // Generated by generate_tokens.py
    input_ids_ = torch::tensor({
        {49406, 871, 14115, 49407, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
        {49406, 4287, 1662, 1094, 541, 9401, 49407, 0, 0, 0, 0, 0, 0, 0, 0, 0},
        {49406, 1573, 5392, 49407, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}
    }, torch::kInt64).to(device_);
    
    attention_mask_ = torch::tensor({
        {1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
        {1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0},
        {1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}
    }, torch::kInt64).to(device_);
    
    // Initialize undistortion maps if camera parameters are provided
    if (!camera_params_.empty()) {
        initUndistortMaps();
    }
}

void ObjectDetector::initUndistortMaps() {
    cv::initUndistortRectifyMap(
        camera_params_.getCameraMatrix(),
        camera_params_.getDistCoeff(),
        cv::Mat_<double>::eye(3, 3),
        camera_params_.getCameraMatrix(),
        camera_params_.getImageSize(),
        CV_16SC2,
        map1_,
        map2_
    );
}

torch::Tensor ObjectDetector::preprocess(const cv::Mat& image) {
    const std::vector<float> mean = {0.5f, 0.5f, 0.5f};
    const std::vector<float> std = {0.5f, 0.5f, 0.5f};
    
    cv::Mat resized, rgb_image;
    cv::resize(image, resized, cv::Size(768, 768));
    cv::cvtColor(resized, rgb_image, cv::COLOR_BGR2RGB);
    
    // Convert to tensor
    torch::Tensor img_tensor = torch::from_blob(
        rgb_image.data,
        {rgb_image.rows, rgb_image.cols, 3},
        torch::kByte
    );
    
    // Permute to [C, H, W] and convert to float
    img_tensor = img_tensor.permute({2, 0, 1}).to(torch::kFloat32) / 255.0;
    
    // Normalize
    for (int i = 0; i < 3; ++i) {
        img_tensor[i] = (img_tensor[i] - mean[i]) / std[i];
    }
    
    // Add batch dimension
    return img_tensor.unsqueeze(0);
}

std::vector<torch::Tensor> ObjectDetector::runModel(const cv::Mat& image) {
    torch::NoGradGuard no_grad;
    
    // Preprocess and move to device
    torch::Tensor pixel_values = preprocess(image).to(device_);
    
    // Prepare inputs
    std::vector<torch::jit::IValue> inputs;
    inputs.push_back(input_ids_);
    inputs.push_back(pixel_values);
    inputs.push_back(attention_mask_);
    
    // Run inference
    auto outputs = model_.forward(inputs);
    auto output_tuple = outputs.toTuple()->elements();
    
    // Extract logits and predicted boxes
    auto logits = output_tuple[0].toTensor();
    auto pred_boxes = output_tuple[1].toTensor();
    
    return {logits, pred_boxes};
}

std::vector<DetectionResult> ObjectDetector::detect(const cv::Mat& image, bool undistort) {
    if (!enabled_ || empty()) {
        return {};
    }
    
    cv::Mat processed_image = image;
    
    // Apply undistortion if requested and camera params available
    if (undistort && !camera_params_.empty()) {
        cv::remap(image, processed_image, map1_, map2_, cv::INTER_LINEAR);
    }
    
    // Run model
    auto outputs = runModel(processed_image);
    auto logits = outputs[0];
    auto boxes = outputs[1];
    
    // Convert logits to probabilities
    torch::Tensor scores = torch::softmax(logits.squeeze(0), 1);
    
    std::vector<DetectionResult> results;
    
    int img_width = processed_image.cols;
    int img_height = processed_image.rows;
    
    // Process each detection
    for (int i = 0; i < scores.size(0); i++) {
        auto max_result = scores[i].max(0);
        float confidence = std::get<0>(max_result).item<float>();
        int class_idx = std::get<1>(max_result).item<int>();
        
        // Skip background class (class_idx == 0) and low confidence detections
        if (class_idx != 0 && confidence > confidence_threshold_) {
            auto box = boxes[0][i];
            
            // Extract normalized coordinates
            float x_center = box[0].item<float>();
            float y_center = box[1].item<float>();
            float width = box[2].item<float>();
            float height = box[3].item<float>();
            
            // Convert to pixel coordinates
            int x1 = static_cast<int>((x_center - width / 2.0f) * img_width);
            int y1 = static_cast<int>((y_center - height / 2.0f) * img_height);
            int x2 = static_cast<int>((x_center + width / 2.0f) * img_width);
            int y2 = static_cast<int>((y_center + height / 2.0f) * img_height);
            
            // Clamp to image boundaries
            x1 = std::max(0, std::min(x1, img_width - 1));
            y1 = std::max(0, std::min(y1, img_height - 1));
            x2 = std::max(0, std::min(x2, img_width - 1));
            y2 = std::max(0, std::min(y2, img_height - 1));
            
            cv::Rect bbox(x1, y1, x2 - x1, y2 - y1);
            
            // Get class name
            std::string class_name = (static_cast<size_t>(class_idx) < class_names_.size()) 
                                     ? class_names_[class_idx] 
                                     : "unknown";
            
            results.emplace_back(class_idx, class_name, bbox, confidence);
        }
    }
    
    // Apply Non-Maximum Suppression to remove overlapping detections
    results = applyNMS(results, 0.5f);
    
    return results;
}

std::vector<DetectionResult> ObjectDetector::applyNMS(const std::vector<DetectionResult>& detections,
                                                      float nms_threshold) {
    if (detections.empty()) {
        return {};
    }
    
    // Group detections by class
    std::map<int, std::vector<size_t>> class_indices;
    for (size_t i = 0; i < detections.size(); i++) {
        class_indices[detections[i].class_id].push_back(i);
    }
    
    std::vector<DetectionResult> result;
    
    // Apply NMS per class
    for (const auto& pair : class_indices) {
        const std::vector<size_t>& indices = pair.second;
        
        // Sort by confidence (descending)
        std::vector<size_t> sorted_indices = indices;
        std::sort(sorted_indices.begin(), sorted_indices.end(),
                  [&detections](size_t i1, size_t i2) {
                      return detections[i1].confidence > detections[i2].confidence;
                  });
        
        std::vector<bool> suppressed(sorted_indices.size(), false);
        
        for (size_t i = 0; i < sorted_indices.size(); i++) {
            if (suppressed[i]) continue;
            
            const cv::Rect& box1 = detections[sorted_indices[i]].bounding_box;
            result.push_back(detections[sorted_indices[i]]);
            
            // Suppress overlapping boxes
            for (size_t j = i + 1; j < sorted_indices.size(); j++) {
                if (suppressed[j]) continue;
                
                const cv::Rect& box2 = detections[sorted_indices[j]].bounding_box;
                
                // Calculate IoU (Intersection over Union)
                int intersection_x1 = std::max(box1.x, box2.x);
                int intersection_y1 = std::max(box1.y, box2.y);
                int intersection_x2 = std::min(box1.x + box1.width, box2.x + box2.width);
                int intersection_y2 = std::min(box1.y + box1.height, box2.y + box2.height);
                
                int intersection_width = std::max(0, intersection_x2 - intersection_x1);
                int intersection_height = std::max(0, intersection_y2 - intersection_y1);
                int intersection_area = intersection_width * intersection_height;
                
                int box1_area = box1.width * box1.height;
                int box2_area = box2.width * box2.height;
                int union_area = box1_area + box2_area - intersection_area;
                
                float iou = (union_area > 0) ? static_cast<float>(intersection_area) / union_area : 0.0f;
                
                if (iou > nms_threshold) {
                    suppressed[j] = true;
                }
            }
        }
    }
    
    return result;
}

void ObjectDetector::drawDetections(cv::Mat& image, const std::vector<DetectionResult>& results) const {
    for (const auto& result : results) {
        const cv::Rect& bbox = result.bounding_box;
        
        // Draw rectangle
        cv::rectangle(image, bbox, cv::Scalar(0, 255, 0), 2);
        
        // Prepare label text
        std::string label = result.class_name + " " + 
                           std::to_string(static_cast<int>(result.confidence * 100)) + "%";
        
        // Get text size for background
        int baseline;
        cv::Size text_size = cv::getTextSize(label, cv::FONT_HERSHEY_SIMPLEX, 0.5, 1, &baseline);
        
        // Draw filled rectangle for text background
        cv::rectangle(image, 
                     cv::Point(bbox.x, bbox.y - text_size.height - baseline),
                     cv::Point(bbox.x + text_size.width, bbox.y),
                     cv::Scalar(0, 255, 0),
                     cv::FILLED);
        
        // Draw label text
        cv::putText(image, label,
                   cv::Point(bbox.x, bbox.y - 5),
                   cv::FONT_HERSHEY_SIMPLEX,
                   0.5,
                   cv::Scalar(0, 0, 0),
                   1);
    }
}

void ObjectDetector::setEnabled(bool enabled) {
    enabled_ = enabled;
}

bool ObjectDetector::isEnabled() const {
    return enabled_;
}

void ObjectDetector::setConfidenceThreshold(float threshold) {
    confidence_threshold_ = threshold;
}

float ObjectDetector::getConfidenceThreshold() const {
    return confidence_threshold_;
}

bool ObjectDetector::empty() const {
    return class_names_.empty();
}

} // namespace ObjDet
